## Dependencies
- Hadoop-1.0.4
- python 2.7+

## Configuration
Before you run the project, there are a few configurations in run.sh that need to be set up.

1. Edit the number of records and partitions in the first job. `-n` proceeds the number of records while `-p` proceeds the number or partitions. e.g.

		-file src/estimate_mapper.py    -mapper 'python estimate_mapper.py -n 10000 -p 3' \
		-file src/estimate_reducer.py   -reducer 'python estimate_reducer.py -p 3' \

Change the 10000 and 3 to whatever is appropriate.

2. Edit the number of reducer in the second job. Change the 3 in

		-D mapred.reduce.tasks=3 \

to whatever is appropriate.


## Other assumptions

We will create an `estimate_out` and an `materialize_out` direcotry in the home directory on HDFS to store intermediate data, so please make sure there are no conflicts before you run the project.


## How to run the project
Use

	$ ./run.sh /path/to/input /path/to/output

The paths should be on HDFS.


## About
- Author of this README: Joyee Cheung (joyeec9h3@gmail.com)
- Github repo: https://github.com/joyeec9h3/tscube  
   (currently private, will be opened after deadline.)
